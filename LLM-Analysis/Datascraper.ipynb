{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the sentiment analysis on the emotions dataset\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "with open('Emotions_dataset.csv') as file:\n",
    "    data = file.read().split('\\n')\n",
    "# Randomly select a journal entry\n",
    "line = data[random.randint(0, len(data))]\n",
    "print(line)\n",
    "line = line.split(',', 2)\n",
    "jnl_entry = line[2]\n",
    "sentiment = analyzer.polarity_scores(jnl_entry)\n",
    "print(sentiment['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the embedding model\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding = emb_model.encode(jnl_entry)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON objects from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the csv file.\n",
    "with open('Emotions_dataset.csv', 'r') as file:\n",
    "    data = file.read().split('\\n')\n",
    "# Removing the header from the data.\n",
    "data = data[1:]\n",
    "snt_analyzer = SentimentIntensityAnalyzer()\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Splitting the data into the respective columns and storing them in a list of dictionaries.\n",
    "entries = []\n",
    "for line in data:\n",
    "    line = line.split(',', 2)\n",
    "    if not (len(line) < 2):\n",
    "       jnl_entry = line[2]\n",
    "       # Getting the sentiment score of the journal entry.\n",
    "       sentiment = snt_analyzer.polarity_scores(jnl_entry)\n",
    "       # Embedding the journal entry.\n",
    "       embedding = emb_model.encode(jnl_entry)\n",
    "       entry = {\n",
    "           'id' : \"jnl_\" + line[0],\n",
    "           'emotion' : line[1],\n",
    "           'journal_entry' : line[2],\n",
    "           'sentiment_score' : sentiment['compound'],\n",
    "           'embedding' : embedding.tolist()\n",
    "       }\n",
    "       entries.append(entry)\n",
    "# Writing the data to a json file.\n",
    "with open('Emotions_dataset.json', 'w') as file:\n",
    "    file.write(json.dumps(entries, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the embedded data in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the vector data storage\n",
    "DIMENSIONS = 384\n",
    "# Creating an instance of the faiss index.\n",
    "index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "with open('Emotions_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "line = data[0]\n",
    "embedding = line['embedding']\n",
    "index.add(np.array([embedding]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m Dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m384\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Creating an instance of the faiss index.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241m.\u001b[39mIndexFlatL2(DIMENSIONS)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotions_dataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     jnl_entries \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faiss' is not defined"
     ]
    }
   ],
   "source": [
    "Dimensions = 384\n",
    "# Creating an instance of the faiss index.\n",
    "index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "with open('Emotions_dataset.json', 'r') as file:\n",
    "    jnl_entries = json.load(file)\n",
    "embeddings = np.array([entry['embedding'] for entry in data])\n",
    "index.add(embeddings)\n",
    "def search(query, index, emb_model, jnl_entries, k=5):\n",
    "    '''This function takes the user's query and returns the top k journal entries that are similar to the query.'''\n",
    "    query_embedding = np.array(emb_model.encode(query)).reshape(1, -1)\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    return [{'journal_entry':jnl_entries[i]['journal_entry'], 'emotion': jnl_entries[i]['emotion']} for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"I'm feeling really stressed with my workload.\"\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "search_results = search(user_query, index, emb_model, jnl_entries)\n",
    "for result in search_results:\n",
    "    print(result['emotion'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
