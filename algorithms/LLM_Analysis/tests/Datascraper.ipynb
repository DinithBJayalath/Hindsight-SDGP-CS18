{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the sentiment analysis on the emotions dataset\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "with open('Emotions_dataset.csv') as file:\n",
    "    data = file.read().split('\\n')\n",
    "# Randomly select a journal entry\n",
    "line = data[random.randint(0, len(data))]\n",
    "print(line)\n",
    "line = line.split(',', 2)\n",
    "jnl_entry = line[2]\n",
    "sentiment = analyzer.polarity_scores(jnl_entry)\n",
    "print(sentiment['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the embedding model\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding = emb_model.encode(jnl_entry)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating JSON objects from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the csv file.\n",
    "with open('Emotions_dataset.csv', 'r') as file:\n",
    "    data = file.read().split('\\n')\n",
    "# Removing the header from the data.\n",
    "data = data[1:]\n",
    "snt_analyzer = SentimentIntensityAnalyzer()\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Splitting the data into the respective columns and storing them in a list of dictionaries.\n",
    "entries = []\n",
    "for line in data:\n",
    "    line = line.split(',', 2)\n",
    "    if not (len(line) < 2):\n",
    "       jnl_entry = line[2]\n",
    "       # Getting the sentiment score of the journal entry.\n",
    "       sentiment = snt_analyzer.polarity_scores(jnl_entry)\n",
    "       # Embedding the journal entry.\n",
    "       embedding = emb_model.encode(jnl_entry)\n",
    "       entry = {\n",
    "           'id' : \"jnl_\" + line[0],\n",
    "           'emotion' : line[1],\n",
    "           'journal_entry' : line[2],\n",
    "           'sentiment_score' : sentiment['compound'],\n",
    "           'embedding' : embedding.tolist()\n",
    "       }\n",
    "       entries.append(entry)\n",
    "# Writing the data to a json file.\n",
    "with open('Emotions_dataset.json', 'w') as file:\n",
    "    file.write(json.dumps(entries, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the embedded data in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the vector data storage\n",
    "DIMENSIONS = 384\n",
    "# Creating an instance of the faiss index.\n",
    "index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "with open('Emotions_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "line = data[0]\n",
    "embedding = line['embedding']\n",
    "index.add(np.array([embedding]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 384\n",
    "# Creating an instance of the faiss index.\n",
    "index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "with open('Emotions_dataset.json', 'r') as file:\n",
    "    jnl_entries = json.load(file)\n",
    "embeddings = np.array([entry['embedding'] for entry in jnl_entries])\n",
    "index.add(embeddings)\n",
    "def search(query, index, emb_model, jnl_entries, k=10):\n",
    "    '''This function takes the user's query and returns the top k journal entries that are similar to the query.'''\n",
    "    query_embedding = np.array(emb_model.encode(query)).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [{'journal_entry':jnl_entries[i]['journal_entry'], 'emotion': jnl_entries[i]['emotion'], 'sentiment_score':jnl_entries[i]['sentiment_score']} for i in indices[0]]\n",
    "    return results, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Because ridiculously attractive people also have self esteem issues and depression Guess what honey The world sucks for everyone Thereâ€™s no escape We will all die in misery and alone\"\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "search_results, distances  = search(user_query, index, emb_model, jnl_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the index and the retrieved context to generate information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community tiktoken langchain-openai langchainhub langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hopelessness'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the generation of the prompt\n",
    "# List of emotions to choose from\n",
    "EMOTIONAL_STATES = [\"Hopeful\", \"Anxious\", \"Inspired\", \"Overwhelmed\", \"Peaceful\", \"Frustrated\", \"Curious\", \"Uncertain\", \"Hopelessness\"]\n",
    "# Prompt template\n",
    "template = \"\"\"Give the most relevant emotion to the following journal entry based on the sentiment score and the mapped emotions from the given context.\n",
    "context: {context}\n",
    "journal entry: {journal_entry}\n",
    "Note: only choose from the following emotions and only output that emotion: {EMOTIONAL_STATES}\"\"\"\n",
    "# Add the template to the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name = \"gpt-4o-mini\", temperature = 0)\n",
    "# Combining the prompt and the language model\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({'context': search_results, 'journal_entry': user_query, 'EMOTIONAL_STATES': EMOTIONAL_STATES})\n",
    "# Extracting the emotion from the full response\n",
    "emotion = response.content\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved version of prompt generation\n",
    "# TODO: find a way to provide the context to the model with out retrieving it separately\n",
    "# DIMENSIONS = 384\n",
    "# Creating an instance of the faiss index.\n",
    "# index = faiss.IndexFlatL2(DIMENSIONS)\n",
    "with open('Emotions_dataset.json', 'r') as file:\n",
    "    jnl_entries = json.load(file)\n",
    "documents = [Document(page_content=entry['journal_entry']) for entry in jnl_entries]\n",
    "# embeddings = np.array([entry['embedding'] for entry in jnl_entries])\n",
    "emb_model = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "# index.add(embeddings)\n",
    "vector_store = FAISS.from_documents(documents=documents, embedding=emb_model)\n",
    "vector_store.save_local('emotions_vector_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hopelessness'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vectors = FAISS.load_local('emotions_vector_store', embeddings=emb_model, allow_dangerous_deserialization=True)\n",
    "retriever = loaded_vectors.as_retriever(k=10)\n",
    "llm = ChatOpenAI(model_name = \"gpt-4o-mini\", temperature = 0)\n",
    "rag_chain = (  \n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke({'context': retriever, 'journal_entry': user_query, 'EMOTIONAL_STATES': EMOTIONAL_STATES})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
